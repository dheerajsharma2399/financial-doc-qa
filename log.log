ValueError: Error raised by inference API HTTP code: 404, {"error":"model \"llama2\" not found, try pulling it first"}
Traceback:
File "E:\Assignment_data\financial-doc-qa\app.py", line 206, in <module>
    vs = build_vectorstore(all_docs, model_provider)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "E:\Assignment_data\financial-doc-qa\app.py", line 115, in build_vectorstore
    return FAISS.from_documents(all_docs, emb)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "E:\Assignment_data\financial-doc-qa\.venv311\Lib\site-packages\langchain_core\vectorstores\base.py", line 835, in from_documents
    return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "E:\Assignment_data\financial-doc-qa\.venv311\Lib\site-packages\langchain_community\vectorstores\faiss.py", line 1039, in from_texts
    embeddings = embedding.embed_documents(texts)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "E:\Assignment_data\financial-doc-qa\.venv311\Lib\site-packages\langchain_community\embeddings\ollama.py", line 209, in embed_documents
    embeddings = self._embed(instruction_pairs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "E:\Assignment_data\financial-doc-qa\.venv311\Lib\site-packages\langchain_community\embeddings\ollama.py", line 197, in _embed
    return [self._process_emb_response(prompt) for prompt in iter_]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "E:\Assignment_data\financial-doc-qa\.venv311\Lib\site-packages\langchain_community\embeddings\ollama.py", line 197, in <listcomp>
    return [self._process_emb_response(prompt) for prompt in iter_]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "E:\Assignment_data\financial-doc-qa\.venv311\Lib\site-packages\langchain_community\embeddings\ollama.py", line 171, in _process_emb_response
    raise ValueError(